---
layout: post
title: "机器学习问题总结"
subtitle: "机器学习"
author: WenlSun"
header-style: text
tag:
  - 面经
---
## 逻辑回归
逻辑回归虽然被称为是回归，但实际上是分类模型，常用于二分类，逻辑回归因其简单、可并行化、可解释性强深受工业届喜欢。**逻辑回归假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来实现对数据进行二分类的问题。**

[逻辑回归面试题汇总(整理)](https://blog.csdn.net/weixin_42933718/article/details/88874376)<br>[逻辑回归的常见面试点总结](https://www.cnblogs.com/ModifyRong/p/7739955.html)

### 逻辑回归的基本假设

逻辑回归的第一个基本假设是**假设数据服从伯努利分布**。在逻辑回归模型中，假设$h_\theta(x)$为样本为正的概率，$1-h_\theta(x)$为样为负的概率，则整个模型可以表示为$h_\theta(x;\theta) = p$。逻辑回归的第二个假设是假设样本为正的概率是$p = \frac{1}{1+e^{-\theta^Tx}}$。所以逻辑回归的最终形式是$h_\theta(x;\theta) =\frac{1}{1+e^{\theta^Tx}}$。

### 逻辑回归的损失函数

逻辑回归的损失函数的通过极大似然估计推导出来的，其形式是对数损失函数，所以从损失函数的角度来看，逻辑回归的损失函数就是对数损失函数。$$L_\theta\left(x\right )= \prod _{i=1}^{m}h_\theta(x^{i};\theta )^{y{i}}*(1-h_\theta(x^{i};\theta))^{1-y^{i}}$$

### 逻辑回归为什么不用均方误差作为损失函数，而是使用极大似然函数求解参数？

1. 其一是因为如果你使用平方损失函数，会发现梯度更新的速度和sigmod函数本身的梯度是很相关的。sigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。
2. 其二是在使用 Sigmoid 函数作为正样本的概率时，同时将平方损失作为损失函数，这时所构造出来的损失函数是非凸的，不容易求解，容易得到其局部最优解。 
3. 而如果使用极大似然，其目标函数就是对数似然函数，该损失函数是关于未知参数的高阶连续可导的凸函数，便于求其全局最优解，而且对数损失函数的梯度更新和sigmod函数本身的梯度是无关的，这样更新的速度是可以自始至终都比较的稳定，其求解参数的速度是比较快的。对数损失函数的梯度更新公式$$\theta _j=\theta _j-\left ( y^{i} -h_\theta (x^{i};\theta ) \right )\ast x^{i}_j$$

### 逻辑回归的求解方法
由于该极大似然函数无法直接求解，我们一般通过对该函数进行梯度下降来不断逼急最优解。常用的有随机梯度下降算法、批梯度下降算法和小批量梯度下降算法。
+ 简单来说 批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。
+ 随机梯度下降是以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。
+ 小批量梯度下降结合了随机梯度下降算法和批梯度下降算法的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。

### 逻辑回归的目的

逻辑回归的目的是对数据进行二分类。逻辑回归作为一个回归(也就是y值是连续的)，如何应用到分类上去呢。y值确实是一个连续的变量。逻辑回归的做法是划定一个阈值，y值大于这个阈值的是一类，y值小于这个阈值的是另外一类。阈值具体如何调整根据实际情况选择。一般会选择0.5做为阈值来划分。

### 逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？

先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。

### 为什么我们还是会在训练的过程当中将高度相关的特征去掉？

1. 去掉高度相关的特征会让模型的可解释性更好;
2. 可以大大提高训练的速度，如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练速度。其次是特征多了，本身就会增大训练时间。

### 逻辑回归为什么要对特征进行离散化？
在工业界，很少直接将连续值作为特征喂给逻辑回归模型，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：
1. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。
2. 离散化后的特征对异常数据有很强的鲁棒性。比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。
3. 逻辑回归属于广义线性模型，表达能力受限，单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型的表达能力，加大拟合。
4. 离散化后可以进行特征交叉(特征组合)，由$M+N$个变量变为$M\times N$个变量，进一步引入非线性，提升表达能力。
5. 特征离散化后，模型会更加稳定。如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。

总结：（1）计算简单；（2）简化模型；（3）增强模型的泛化能力，不受噪声的影响。

### 在逻辑回归模型中，为什么常常需要做特征交叉(特征组合)？

逻辑回归模型属于线性模型，线性模型不能很好的处理非线性特征，特征组合可以引入非线性特征，提升了模型的表达能力。另外，基本特征可以认为是全局建模，组合特征更加精细，是个性化建模，但对全局建模会对部分样本有偏，对每一个样本建模又会导致数据爆炸、过拟合，所以基本特征+特征组合兼顾了全局和个性化。

### 逻辑回归模型是线性模型吗？

1. 逻辑回归模型是一种广义线性模型，它虽然引入了sigmoid函数，是非线性模型，但是本质上还是一个线性回归模型。
2. 逻辑回归和线性回归首先都是广义的线性回归，在本质上没多大区别，区别在于逻辑回归多了个Sigmod函数，使样本映射到`[0,1]`之间的数值，从而来处理分类问题。另外逻辑回归是假设变量服从伯努利分布，线性回归假设变量服从高斯分布。逻辑回归输出的是离散型变量，用于分类，线性回归输出的是连续性的，用于预测。逻辑回归是用最大似然法去计算预测函数中的最优参数值，而线性回归是用最小二乘法去对自变量因变量关系进行拟合。

### 逻辑回归模型的输出值的实际意义是什么？

只有在满足： y服从伯努利分布；η和x之间存在线性关系时，输出值才是概率值。不满足的情况下，得到的输出值，只是置信度。假设 y 是一个服从伯努利分布的二值随机变量。该分布的参数为$\Phi = P(y=1)$。伯努利分布属于指数家族的一种情况，指数分布家族的形式为：$$P(x|\eta)=h(x)exp{\eta^TT(x)-A(\eta)}$$
它告诉我们：对于随机变量x，只要确定三个函数$h(x)$、$T(x)$、$A(\eta)$，就可以确定一类分布。 $\eta$用来确定该类分布的具体参数。从伯努利分布出发，可变形到与指数分布族一样的形式：
$$P(y;\Phi) = \Phi^y(1-\Phi)^{1-y}\\=exp(log\Phi^y(1-\Phi)^{1-y}\\=exp(ylog\Phi+(1-y)(1-\Phi)\\=exp(log\frac{\Phi}{1-\Phi}y+log(1-\Phi))​$$
对应上面提到的三个函数：
$$A(\eta)=-log(1-\Phi)=log(1+e^\eta)\\h(y) = 1\\T(y) = y​$$
$\eta​$和$\Phi​$之间的关系：$$\eta = log\frac{\Phi}{1-\Phi}\\\Phi = \frac{1}{1+e^{-\eta}}​$$
因此，伯努利分布可以改写为指数分布族的形式，而且，伯努利分布的参数$\Phi​$与$\eta​$之间，还满足sigmoid函数的关系。如果能找到x和$\eta​$之间的关系，就找到了x和$\Phi​$之间的关系。假设$\eta​$和x之间存在线性的关系，即：$\eta = \theta x​$。将$\Phi​$作为预测值。$\Phi​$既是伯努利分布的唯一参数，也是该分布的期望，也是逻辑回归的输出。至此，找到了我们要用的模型的样子，也就是逻辑回归。

如果你的情况满足上面所说的两个假设，那么你训练模型的过程，就确实是在对概率进行建模。但是这两个假设并不是那么容易满足的。所以，很多情况下，我们得出的逻辑回归输出值，无法当作真实的概率，只能作为置信度来使用。

### 逻辑回归模型的优缺点(偏向工业界)

#### 优点
1. 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。
2. 模型效果不错。在工程上是可以接受的，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。
3. 训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。
4. 资源占用小,尤其是内存。因为只需要存储各个维度的特征值。
5. 方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数(置信度)。

#### 缺点
1. 准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。
2. 很难处理数据不平衡的问题。
3. 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题。
4. 逻辑回归本身无法筛选特征。有时候，我们会用`gbdt`来筛选特征，然后再上逻辑回归。

