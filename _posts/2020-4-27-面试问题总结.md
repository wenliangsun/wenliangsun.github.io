---
layout: post
title: "机器学习问题总结"
subtitle: "机器学习"
author: WenlSun"
header-style: text
tag:
  - 面经
---
## 逻辑回归
逻辑回归虽然被称为是回归，但实际上是分类模型，常用于二分类，逻辑回归因其简单、可并行化、可解释性强深受工业届喜欢。**逻辑回归假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来实现对数据进行二分类的问题。**

[逻辑回归面试题汇总(整理)](https://blog.csdn.net/weixin_42933718/article/details/88874376)<br>[逻辑回归的常见面试点总结](https://www.cnblogs.com/ModifyRong/p/7739955.html)

### 逻辑回归的基本假设

逻辑回归的第一个基本假设是**假设数据服从伯努利分布**。在逻辑回归模型中，假设$h_\theta(x)$为样本为正的概率，$1-h_\theta(x)$为样为负的概率，则整个模型可以表示为$h_\theta(x;\theta) = p$。逻辑回归的第二个假设是假设样本为正的概率是$p = \frac{1}{1+e^{-\theta^Tx}}$。所以逻辑回归的最终形式是$h_\theta(x;\theta) =\frac{1}{1+e^{-\theta^Tx}}$。

### 逻辑回归的损失函数

逻辑回归的损失函数的通过极大似然估计推导出来的，其形式是对数损失函数，所以从损失函数的角度来看，逻辑回归的损失函数就是对数损失函数。

$$
L_\theta\left(x\right )= \prod _{i=1}^{m}h_\theta(x^{i};\theta )^{y{i}}*(1-h_\theta(x^{i};\theta))^{1-y^{i}}
$$

### 逻辑回归为什么不用均方误差作为损失函数，而是使用极大似然函数求解参数？

1. 其一是因为如果你使用平方损失函数，会发现梯度更新的速度和sigmod函数本身的梯度是很相关的。sigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。
2. 其二是在使用 Sigmoid 函数作为正样本的概率时，同时将平方损失作为损失函数，这时所构造出来的损失函数是非凸的，不容易求解，容易得到其局部最优解。 
3. 而如果使用极大似然，其目标函数就是对数似然函数，该损失函数是关于未知参数的高阶连续可导的凸函数，便于求其全局最优解，而且对数损失函数的梯度更新和sigmod函数本身的梯度是无关的，这样更新的速度是可以自始至终都比较的稳定，其求解参数的速度是比较快的。对数损失函数的梯度更新公式

$$
\theta _j=\theta _j-\left ( y^{i} -h_\theta (x^{i};\theta ) \right )\ast x^{i}_j
$$

### 逻辑回归的求解方法
由于该极大似然函数无法直接求解，我们一般通过对该函数进行梯度下降来不断逼急最优解。常用的有随机梯度下降算法、批梯度下降算法和小批量梯度下降算法。
+ 简单来说 批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。
+ 随机梯度下降是以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。
+ 小批量梯度下降结合了随机梯度下降算法和批梯度下降算法的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。

### 逻辑回归的目的

逻辑回归的目的是对数据进行二分类。逻辑回归作为一个回归(也就是y值是连续的)，如何应用到分类上去呢。y值确实是一个连续的变量。逻辑回归的做法是划定一个阈值，y值大于这个阈值的是一类，y值小于这个阈值的是另外一类。阈值具体如何调整根据实际情况选择。一般会选择0.5做为阈值来划分。

### 逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？

先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。

### 为什么我们还是会在训练的过程当中将高度相关的特征去掉？

1. 去掉高度相关的特征会让模型的可解释性更好;
2. 可以大大提高训练的速度，如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练速度。其次是特征多了，本身就会增大训练时间。

### 逻辑回归为什么要对特征进行离散化？
在工业界，很少直接将连续值作为特征喂给逻辑回归模型，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：
1. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。
2. 离散化后的特征对异常数据有很强的鲁棒性。比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。
3. 逻辑回归属于广义线性模型，表达能力受限，单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型的表达能力，加大拟合。
4. 离散化后可以进行特征交叉(特征组合)，由$M+N$个变量变为$M\times N$个变量，进一步引入非线性，提升表达能力。
5. 特征离散化后，模型会更加稳定。如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。

总结：（1）计算简单；（2）简化模型；（3）增强模型的泛化能力，不受噪声的影响。

### 在逻辑回归模型中，为什么常常需要做特征交叉(特征组合)？

逻辑回归模型属于线性模型，线性模型不能很好的处理非线性特征，特征组合可以引入非线性特征，提升了模型的表达能力。另外，基本特征可以认为是全局建模，组合特征更加精细，是个性化建模，但对全局建模会对部分样本有偏，对每一个样本建模又会导致数据爆炸、过拟合，所以基本特征+特征组合兼顾了全局和个性化。

### 逻辑回归模型是线性模型吗？

1. 逻辑回归模型是一种广义线性模型，它虽然引入了sigmoid函数，是非线性模型，但是本质上还是一个线性回归模型。
2. 逻辑回归和线性回归首先都是广义的线性回归，在本质上没多大区别，区别在于逻辑回归多了个Sigmod函数，使样本映射到`[0,1]`之间的数值，从而来处理分类问题。另外逻辑回归是假设变量服从伯努利分布，线性回归假设变量服从高斯分布。逻辑回归输出的是离散型变量，用于分类，线性回归输出的是连续性的，用于预测。逻辑回归是用最大似然法去计算预测函数中的最优参数值，而线性回归是用最小二乘法去对自变量因变量关系进行拟合。

### 逻辑回归模型的输出值的实际意义是什么？

只有在满足： y服从伯努利分布；η和x之间存在线性关系时，输出值才是概率值。不满足的情况下，得到的输出值，只是置信度。假设 y 是一个服从伯努利分布的二值随机变量。该分布的参数为$\Phi = P(y=1)$。伯努利分布属于指数家族的一种情况，指数分布家族的形式为：$$P(x|\eta)=h(x)exp(\eta^TT(x)-A(\eta))$$
它告诉我们：对于随机变量x，只要确定三个函数$h(x)$、$T(x)$、$A(\eta)$，就可以确定一类分布。 $\eta$用来确定该类分布的具体参数。从伯努利分布出发，可变形到与指数分布族一样的形式：
$$
P(y;\Phi) = \Phi^y(1-\Phi)^{1-y}\\=exp(log\Phi^y(1-\Phi)^{1-y}\\=exp(ylog\Phi+(1-y)(1-\Phi)\\=exp(log\frac{\Phi}{1-\Phi}y+log(1-\Phi))
$$

对应上面提到的三个函数：

$$
A(\eta)=-log(1-\Phi)=log(1+e^\eta)\\h(y) = 1\\T(y) = y
$$

$\eta​$和$\Phi​$之间的关系：

$$
\eta = log\frac{\Phi}{1-\Phi}\\\Phi = \frac{1}{1+e^{-\eta}}
$$

因此，伯努利分布可以改写为指数分布族的形式，而且，伯努利分布的参数$\Phi$与$\eta$之间，还满足sigmoid函数的关系。如果能找到x和$\eta$之间的关系，就找到了x和$\Phi$之间的关系。假设$\eta$和x之间存在线性的关系，即：$\eta = \theta x$。将$\Phi$作为预测值。$\Phi​$既是伯努利分布的唯一参数，也是该分布的期望，也是逻辑回归的输出。至此，找到了我们要用的模型的样子，也就是逻辑回归。

如果你的情况满足上面所说的两个假设，那么你训练模型的过程，就确实是在对概率进行建模。但是这两个假设并不是那么容易满足的。所以，很多情况下，我们得出的逻辑回归输出值，无法当作真实的概率，只能作为置信度来使用。

### 逻辑回归模型的优缺点(偏向工业界)

#### 优点
1. 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。
2. 模型效果不错。在工程上是可以接受的，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。
3. 训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。
4. 资源占用小,尤其是内存。因为只需要存储各个维度的特征值。
5. 方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数(置信度)。

#### 缺点

1. 准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。
2. 很难处理数据不平衡的问题。
3. 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题。
4. 逻辑回归本身无法筛选特征。有时候，我们会用`gbdt`来筛选特征，然后再上逻辑回归。

### 正则化

正则化是一个通用的算法和思想，所有会产生过拟合现象的算法都可以使用正则化来避免过拟合。在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，可以有效提高泛化预测精度。如果模型过于复杂，变量值稍微有点变动，就会引起预测精度问题。正则化之所以有效，就是因为其降低了特征的权重，使得模型更为简单。正则化一般会采用 $L_1$ 范式或者 $L_2$ 范式。

#### $L_1$正则化 (Lasso回归)

相当于为模型添加了一个先验知识，即w服从零均值拉普拉斯分布

$$
f(w|\mu,b)=\frac{1}{2b}exp(-\frac{|w-\mu|}{b})
$$

由于引入了先验，似然函数变为

![](/img/面试问题总结/L11.png)

取 log 再取负，得到目标函数

![](/img/面试问题总结/L12.png)

等价于原始损失函数的后面加上了 $L_1​$ 正则，因此 $L_1​$ 正则的本质其实是为模型增加了“**模型参数服从零均值拉普拉斯分布**”这一先验知识。

#### $L_2$正则化 (岭回归)

相当于为模型添加了一个先验知识：即 w 服从零均值正态分布。

![](/img/面试问题总结/L21.png)

由于引入了先验知识，所以似然函数这样写：

![](/img/面试问题总结/L22.png)

取 ln 再取负，得到目标函数：

![](/img/面试问题总结/L23.png)

等价于原始的损失函数后面加上了 $L_2$正则，因此 $L_2$ 正则的本质其实是为模型增加了**模型参数服从零均值正态分布**这一先验知识。

#### $L_1$和$L_2$正则化的区别

$L_1$ 正则化增加了所有权重 w 参数的绝对值之和逼迫更多 w 为零，也就是变稀疏。我们对稀疏规则趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，大部分特征 $x_i$ 都是和最终的输出 $y_i$ 没有关系或者不提供任何信息的。在最小化目标函数的时候考虑 $x_i$ 这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的特征权重反而会被考虑，从而干扰了对正确 $y_i$ 的预测。$L_1$ 正则化的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些无用的特征，也就是把这些特征对应的权重置为0。

$L_2$正则化中增加所有权重 w 参数的平方之和，逼迫所有 w 尽可能趋向零但不为零（$L_2$的导数趋于零）。因为在未加入 $L_2$正则化发生过拟合时，拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大，在某些很小的区间里，函数值的变化很剧烈，也就是某些 w 值非常大。为此，$L_2$正则化的加入就惩罚了权重变大的趋势。

$L_1$正则化指权值向量中各个元素的绝对值之和，$L_2$正则化是指权值向量中各个元素的平方和再求平方根。$L_1$正则会产生稀疏解，$L_2$正则会产生比较小的解。以二维为例，$L_1$正则化项和误差项的交点常出现在坐标轴上，是个菱形，$w_1$或$w_2$为0，即权值向量中有零值元素，而$L_2$正则化项与误差项的交点常出现在某个象限中，是个圆，$w_1$和$w_2$均非0。

![](/img/面试问题总结/L1L2.jpg)

### 工程上，怎么实现逻辑回归的并行化？有哪些并行化工具？

逻辑回归的并行化最主要的就是**对目标函数梯度计算的并行化**。目标函数的梯度向量计算中只需要进行向量间的点乘和相加，可以很容易将每个迭代过程拆分成相互独立的计算步骤，由不同的节点进行独立计算，然后归并计算结果。算法的并行化有两种：**无损并行化和有损并行化**。
基于Batch的算法(Batch-GD, LBFGS, OWLQN)都是可以进行无损的并行化的。而基于SGD的算法（Ad Predictor， FTRL－Proximal）都只能进行有损的并行化。

1. 无损的并行化：算法天然可以并行，并行只是提高了计算的速度和解决问题的规模，但和正常执行的结果是一样的。
2. 有损的并行化：算法本身不是天然并行的，需要对算法做一些近似来实现并行化，这样并行化之后的双方和正常执行的结果并不一致，但是相似的。

并行化的工具有**MPI**和**OpenMP**。

### 逻辑回归与其他模型的比较

#### 与线性回归模型

逻辑回归是在线性回归的基础上加了一个 sigmoid 函数（非线形）映射，使得逻辑回归称为了一个优秀的分类算法。本质上来说，两者都属于广义线性模型，但他们两个要解决的问题不一样，逻辑回归解决的是分类问题，输出的是离散值，线性回归解决的是回归问题，输出的连续值。

sigmoid的作用：
+ 线性回归是在实数域范围内进行预测，而分类范围则需要在 [0,1]，逻辑回归减少了预测范围；
+ 线性回归在实数域上敏感度一致，而逻辑回归在 0 附近敏感，在远离 0 点位置不敏感，这个的好处就是模型更加关注分类边界，可以增加模型的鲁棒性。

#### 与最大熵模型

逻辑回归和最大熵模型本质上没有区别，最大熵在解决二分类问题时就是逻辑回归，在解决多分类问题时就是多项逻辑回归。

#### 与SVM模型

相同点：<br>
1. 都是分类算法，本质上都是寻找划分超平面
2. 都是监督学习算法
3. 都是判别式模型
4. 都可以通过核技巧的方法对非线性情况进行分类
5. 都能减少离群点的影响

不同点：<br>
1. 损失函数不同，LR是对数损失函数，SVM是合页损失函数。
2. LR对异常值敏感，SVM对异常值不敏感。解释：支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局。LR模型找到的那个超平面，是尽量让所有点都远离他，而SVM寻找的那个超平面，是只让最靠近中间分割线的那些点尽量远离，即只用到那些支持向量的样本。 支持向量机改变非支持向量样本并不会引起决策面的变化。逻辑回归中改变任何样本都会引起决策面的变化。
3. 对非线性问题的处理方式不同。解释：LR主要通过特征构造，特征组合(特征交叉)、特征离散化来处理非线性问题。SVM通常采用核函数来高效处理非线性问题。
4. 理论基础不一样。LR基于统计，而SVM基于严格的数学推导。
5. 输出不同。LR可以对每个样本点给出类别判断的概率值(或置信度)，SVM无法做到。
6. 计算复杂度不同。对于海量数据，SVM的效率较低，LR的效率比较高。解释：当样本较少，特征维数较低时，SVM和LR的运行时间均比较短，SVM较短一些。准确率的话，LR明显比SVM要高。当样本稍微增加些时，SVM运行时间开始增长，但是准确率赶超了LR。SVM时间虽长，但在可接受范围内。当数据量增长到20000时，特征维数增长到200时，SVM的运行时间剧烈增加，远远超过了LR的运行时间。但是准确率却和LR相差无几。(这其中主要原因是大量非支持向量参与计算，造成SVM的二次规划问题)
7. 防止过拟合的能力不同。解释：SVM模型中内含了L2正则，可有效防止过拟合。LR要自己添加正则项。
8. 对数据要求不同。解释：**SVM依赖于数据表达出的距离测度，所以需要对数据进行标准化处理**，而LR不需要。
9. SVM会用核函数而LR一般不用核函数。
10. SVM自带结构风险最小化，LR则是经验风险最小化。

## 支持向量机

### SVM的原理，

支持向量机是一种二分类模型，它的**基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器**。学习策略是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。
+ 当训练数据线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机。
+ 当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机。
+ 当训练数据线性不可分时，通过使用核技巧以及软间隔最大化，学习一个非线性支持向量机。

### SVM为什么采用间隔最大化？

1. 当训练数据线性可分时，就会存在无数个分离超平面可以将训练数据正确分开 (感知机模型)。
2. 线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解释唯一的。
3. 另一方面，通过最大化间隔求得的分离超平面所产生的的分类结果是最鲁棒的，对未知实例的泛化能力最强。

可以借此机会阐述一下几何间隔以及函数间隔的关系。

### SVM推导中的函数间隔和几何间隔

#### 函数间隔

#### 几何间隔

### SVM的推导？

函数间隔 -> 几何间隔 -> 几何间隔最大化 -> 函数间隔最大化 -> 拉格朗日函数 -> 求解对偶问题 -> SMO算法
手推！！！

线性可分 - 硬间隔最大化 - 线性可分支持向量机
线性近似可分 - 软间隔最大化 - 线性可分支持向量机
线性不可分 - 核函数 - 非线性支持向量机

### SVM为什么转换为对偶问题求解？

+ 转换对偶问题求解减少算法复杂度，使得算法更高效，从求解w,b转换成求解$\alpha$；
+ 不等式约束是优化问题中的难点，求解对偶问题可以将支持向量机原问题中的不等式约束转换成等式约束；
+ 支持向量机在解决非线性可分问题时，需要将数据映射到高维空间，但映射函数的具体形式不容易确定，而转换成对偶问题时，可以使用核函数来解决这个问题。

### SVM 为什么要引入核函数？

1. 当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在高维特征空间内线性可分。
2. 引入这样的映射后，在所要求解的对偶问题的求解中，无需求解真正的映射函数，而只需要知道其核函数。
3. 核函数的引入避免了“维数灾难”，大大减小了计算量。
4. 无需知道非线性变换函数$\Phi$的形式和参数，只需要给定具体的核函数即可，这样使得求解的难度大大降低。
5. 核函数的形式和参数变换会隐式的改变从输入空间到特征空间的映射，进而对特征空间的性质产生影响，最终改变各种核函数方法的性能。

### SVM核函数的选择依据，以及各种核函数的区别？

一般选择线性核和高斯核，也就是线性核和RBF核。**需要注意的是需要对数据归一化处理**。
1. 线性核：主要用于线性可分的情形，参数少，速度快，对于一般数据，分类效果已经很理想了。
2. RBF核：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。可通过交叉验证来寻找合适的参数，不过这个过程比较耗时。如果特征的数量很大，跟样本数量差不多，这时候选用线性核的SVM；如果特征的数量比较小，样本数量一般，不算大也不算小，选用高斯核的SVM。
3. 对于某些参数，RBF和sigmoid具有相似的性能。
4. 其他核函数：cosin核，Chi-squared核。

### SVM如何处理多分类问题？

一般有两种做法：一种是直接法，直接在目标函数上修改，将多个分类面的参数求解合并到一个最优化问题里面，看似简单但是计算量却非常的大。另外一种做法是间接法：对训练器进行组合。其中比较典型的有一对一，和一对多。

+ **一对多**：就是对每个类都训练出一个分类器，由于svm是二分类，因此将目标类作为一类，其余类作为另外一类。这样针对k个类可以训练出k个分类器，当有一个新的样本来的时候，用这k个分类器来测试，哪个分类器的概率高，那么这个样本就属于哪一类。这种方法效果不太好，bias比较高。
+ **一对一**：针对任意两个类训练出一个分类器，如果有k类，一共训练出C(2,k) 个分类器，这样当有一个新的样本要来的时候，用这C(2,k)个分类器来测试，每当被判定属于某一类的时候，该类就加一，最后票数最多的类别被认定为该样本的类。

### SVM的主要优势、特点

1. 泛化性能比较好，不容易过拟合(合页损失函数+自带正则)。
2. 可以在较少的数据下取得较好的性能(支持向量)。
3. 存在全局最优解 (严格数学推导, 凸二次规划问题)。
4. 存在高效实现的训练算法(SMO算法)。
5. 可以使用核技巧处理非线性问题。
6. SVM的最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”。

### SVM的主要缺点

1. SVM算法对大规模训练样本难以实施，速度慢。
2. 用SVM解决多分类问题存在困难。
3. 对缺失数据(缺失的特征数据)敏感，对参数和核函数的选择敏感。

### 为什么SVM对缺失数据敏感？

1. 这里说的缺失数据是指缺失某些特征数据、向量数据不完整。
2. 因为SVM没有处理缺失值的策略，而SVM希望样本在特征空间线性可分，所以特征空间的好坏对SVM的性能很重要，缺失特征数据将影响训练结果的好坏。

### SMO算法的求解对偶问题流程？

SMO算法是一种启发式算法，其基本思路是：如果所有变量都满足此优化问题的KKT条件，那么这个最优化问题的解就得到了(因为KKT条件是该最优化问题的充分必要条件)。否则，选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题，这时构建的这个子问题可以通过解析方法求解，大大提高了算法的计算速度，子问题有两个变量，一个是违反KKT最严重的那一个，另一个是由约束条件自动确定。SMO包括两部分：求解两个变量的二次规划的解析方法和选择变量的启发式方法。

变量的选择方法：

**第一个变量的选择**：SMO称第一个变量选择过程为外层循环，选取最违反KKT条件的样本点，在检验过程中，先遍历所有满足条件的$0\le \alpha_i \le C$的样本点(即在间隔边界上的支持向量点)，检验是否满足KKT条件，如果都满足，那么遍历整个训练集。检验他们是否满足KKT条件。
**第二个变量的选择**：称为为内层循环，选择标准为希望能使$\alpha_2$有足够大的变化，一种简单的做法是选择$\alpha_2$,使其对应的|$E_1-E_2$|最大。在特殊情况下，若以上方法找不到$\alpha_2$，则遍历间隔边界上的支持向量点，依次将其对应的变量作为$\alpha_2$试用，直到目标函数有足够的的下降。若找不到合适的$\alpha_2$,那么遍历整个训练集，若仍找不到，则放弃$\alpha_1$,重新找$\alpha_1$。

![](/img/面试问题总结/smo.png)

SMO算法不断地将原二次规划问题分解为只有两个变量的二次规划子问题，并对子问题进行解析求解，知道所有的变量满足KKT条件为止，因为子问题有解析解，所以每次计算子问题都很快，虽然计算子问题的次数很多，但总体还是高效的。

### SVM和逻辑回归的异同

相同点：<br>
1. LR和SVM都是分类算法
2. LR和SVM都是监督学习算法
3. LR和SVM都是判别式模型
4. 如果不考虑核函数，LR和SVM都是线性分类算法，即他们的分类决策面是线性的。(注意：LR也可以核化，但是计算量太大，一般不这么做)

不同点：<br>
1. LR采用-log损失(对数损失函数)，SVM采用合页损失函数(hinge)
2. LR对异常值敏感，SVM对异常值不敏感
3. 计算复杂度不同，对于海量数据，SVM效率较低，LR效率较高
4. 对于非线性问题的处理方式不同
5. SVM的损失自带正则
6. SVM自带结构风险最小化，LR则是经验风险最小化。
7. SVM一般会使用核函数，LR一般不使用核函数。

