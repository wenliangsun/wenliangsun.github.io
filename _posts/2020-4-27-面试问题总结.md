---
layout: post
title: "机器学习问题总结"
subtitle: "机器学习"
author: WenlSun"
header-style: text
tag:
  - 面经
---
## 逻辑回归
逻辑回归虽然被称为是回归，但实际上是分类模型，常用于二分类，逻辑回归因其简单、可并行化、可解释性强深受工业届喜欢。**逻辑回归假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来实现对数据进行二分类的问题。**

[逻辑回归面试题汇总(整理)](https://blog.csdn.net/weixin_42933718/article/details/88874376)<br>[逻辑回归的常见面试点总结](https://www.cnblogs.com/ModifyRong/p/7739955.html)

### 逻辑回归的基本假设

逻辑回归的第一个基本假设是**假设数据服从伯努利分布**。在逻辑回归模型中，假设$h_\theta(x)$为样本为正的概率，$1-h_\theta(x)$为样为负的概率，则整个模型可以表示为$h_\theta(x;\theta) = p$。逻辑回归的第二个假设是假设样本为正的概率是$p = \frac{1}{1+e^{-\theta^Tx}}$。所以逻辑回归的最终形式是$h_\theta(x;\theta) =\frac{1}{1+e^{\theta^Tx}}$。

### 逻辑回归的损失函数

逻辑回归的损失函数的通过极大似然估计推导出来的，其形式是对数损失函数，所以从损失函数的角度来看，逻辑回归的损失函数就是对数损失函数。

$$
L_\theta\left(x\right )= \prod _{i=1}^{m}h_\theta(x^{i};\theta )^{y{i}}*(1-h_\theta(x^{i};\theta))^{1-y^{i}}
$$

### 逻辑回归为什么不用均方误差作为损失函数，而是使用极大似然函数求解参数？

1. 其一是因为如果你使用平方损失函数，会发现梯度更新的速度和sigmod函数本身的梯度是很相关的。sigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。
2. 其二是在使用 Sigmoid 函数作为正样本的概率时，同时将平方损失作为损失函数，这时所构造出来的损失函数是非凸的，不容易求解，容易得到其局部最优解。 
3. 而如果使用极大似然，其目标函数就是对数似然函数，该损失函数是关于未知参数的高阶连续可导的凸函数，便于求其全局最优解，而且对数损失函数的梯度更新和sigmod函数本身的梯度是无关的，这样更新的速度是可以自始至终都比较的稳定，其求解参数的速度是比较快的。对数损失函数的梯度更新公式

$$
\theta _j=\theta _j-\left ( y^{i} -h_\theta (x^{i};\theta ) \right )\ast x^{i}_j
$$

### 逻辑回归的求解方法
由于该极大似然函数无法直接求解，我们一般通过对该函数进行梯度下降来不断逼急最优解。常用的有随机梯度下降算法、批梯度下降算法和小批量梯度下降算法。
+ 简单来说 批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。
+ 随机梯度下降是以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。
+ 小批量梯度下降结合了随机梯度下降算法和批梯度下降算法的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。

### 逻辑回归的目的

逻辑回归的目的是对数据进行二分类。逻辑回归作为一个回归(也就是y值是连续的)，如何应用到分类上去呢。y值确实是一个连续的变量。逻辑回归的做法是划定一个阈值，y值大于这个阈值的是一类，y值小于这个阈值的是另外一类。阈值具体如何调整根据实际情况选择。一般会选择0.5做为阈值来划分。

### 逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？

先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。

### 为什么我们还是会在训练的过程当中将高度相关的特征去掉？

1. 去掉高度相关的特征会让模型的可解释性更好;
2. 可以大大提高训练的速度，如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练速度。其次是特征多了，本身就会增大训练时间。

### 逻辑回归为什么要对特征进行离散化？
在工业界，很少直接将连续值作为特征喂给逻辑回归模型，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：
1. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。
2. 离散化后的特征对异常数据有很强的鲁棒性。比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。
3. 逻辑回归属于广义线性模型，表达能力受限，单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型的表达能力，加大拟合。
4. 离散化后可以进行特征交叉(特征组合)，由$M+N$个变量变为$M\times N$个变量，进一步引入非线性，提升表达能力。
5. 特征离散化后，模型会更加稳定。如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。

总结：（1）计算简单；（2）简化模型；（3）增强模型的泛化能力，不受噪声的影响。

### 在逻辑回归模型中，为什么常常需要做特征交叉(特征组合)？

逻辑回归模型属于线性模型，线性模型不能很好的处理非线性特征，特征组合可以引入非线性特征，提升了模型的表达能力。另外，基本特征可以认为是全局建模，组合特征更加精细，是个性化建模，但对全局建模会对部分样本有偏，对每一个样本建模又会导致数据爆炸、过拟合，所以基本特征+特征组合兼顾了全局和个性化。

### 逻辑回归模型是线性模型吗？

1. 逻辑回归模型是一种广义线性模型，它虽然引入了sigmoid函数，是非线性模型，但是本质上还是一个线性回归模型。
2. 逻辑回归和线性回归首先都是广义的线性回归，在本质上没多大区别，区别在于逻辑回归多了个Sigmod函数，使样本映射到`[0,1]`之间的数值，从而来处理分类问题。另外逻辑回归是假设变量服从伯努利分布，线性回归假设变量服从高斯分布。逻辑回归输出的是离散型变量，用于分类，线性回归输出的是连续性的，用于预测。逻辑回归是用最大似然法去计算预测函数中的最优参数值，而线性回归是用最小二乘法去对自变量因变量关系进行拟合。

### 逻辑回归模型的输出值的实际意义是什么？

只有在满足： y服从伯努利分布；η和x之间存在线性关系时，输出值才是概率值。不满足的情况下，得到的输出值，只是置信度。假设 y 是一个服从伯努利分布的二值随机变量。该分布的参数为$\Phi = P(y=1)$。伯努利分布属于指数家族的一种情况，指数分布家族的形式为：$$P(x|\eta)=h(x)exp{\eta^TT(x)-A(\eta)}$$
它告诉我们：对于随机变量x，只要确定三个函数$h(x)$、$T(x)$、$A(\eta)$，就可以确定一类分布。 $\eta$用来确定该类分布的具体参数。从伯努利分布出发，可变形到与指数分布族一样的形式：

$$
P(y;\Phi) = \Phi^y(1-\Phi)^{1-y}\\=exp(log\Phi^y(1-\Phi)^{1-y}\\=exp(ylog\Phi+(1-y)(1-\Phi)\\=exp(log\frac{\Phi}{1-\Phi}y+log(1-\Phi))
$$

对应上面提到的三个函数：

$$
A(\eta)=-log(1-\Phi)=log(1+e^\eta)\\h(y) = 1\\T(y) = y
$$

$\eta​$和$\Phi​$之间的关系：

$$
\eta = log\frac{\Phi}{1-\Phi}\\\Phi = \frac{1}{1+e^{-\eta}}
$$

因此，伯努利分布可以改写为指数分布族的形式，而且，伯努利分布的参数$\Phi$与$\eta$之间，还满足sigmoid函数的关系。如果能找到x和$\eta$之间的关系，就找到了x和$\Phi$之间的关系。假设$\eta$和x之间存在线性的关系，即：$\eta = \theta x$。将$\Phi$作为预测值。$\Phi​$既是伯努利分布的唯一参数，也是该分布的期望，也是逻辑回归的输出。至此，找到了我们要用的模型的样子，也就是逻辑回归。

如果你的情况满足上面所说的两个假设，那么你训练模型的过程，就确实是在对概率进行建模。但是这两个假设并不是那么容易满足的。所以，很多情况下，我们得出的逻辑回归输出值，无法当作真实的概率，只能作为置信度来使用。

### 逻辑回归模型的优缺点(偏向工业界)

#### 优点
1. 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。
2. 模型效果不错。在工程上是可以接受的，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。
3. 训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。
4. 资源占用小,尤其是内存。因为只需要存储各个维度的特征值。
5. 方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数(置信度)。

#### 缺点

1. 准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。
2. 很难处理数据不平衡的问题。
3. 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题。
4. 逻辑回归本身无法筛选特征。有时候，我们会用`gbdt`来筛选特征，然后再上逻辑回归。

### 正则化

正则化是一个通用的算法和思想，所有会产生过拟合现象的算法都可以使用正则化来避免过拟合。在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，可以有效提高泛化预测精度。如果模型过于复杂，变量值稍微有点变动，就会引起预测精度问题。正则化之所以有效，就是因为其降低了特征的权重，使得模型更为简单。正则化一般会采用 $L_1$ 范式或者 $L_2$ 范式。

#### $L_1$正则化 (Lasso回归)

相当于为模型添加了一个先验知识，即w服从零均值拉普拉斯分布

$$
f(w|\mu,b)=\frac{1}{2b}exp(-\frac{|w-\mu|}{b})
$$

由于引入了先验，似然函数变为

![](/img/面试问题总结/L11.png)

取 log 再取负，得到目标函数

![](/img/面试问题总结/L12.png)

等价于原始损失函数的后面加上了 $L_1​$ 正则，因此 $L_1​$ 正则的本质其实是为模型增加了“**模型参数服从零均值拉普拉斯分布**”这一先验知识。

#### $L_2$正则化 (岭回归)

相当于为模型添加了一个先验知识：即 w 服从零均值正态分布。

![](/img/面试问题总结/L21.png)

由于引入了先验知识，所以似然函数这样写：

![](/img/面试问题总结/L22.png)

取 ln 再取负，得到目标函数：

![](/img/面试问题总结/L23.png)

等价于原始的损失函数后面加上了 $L_2$正则，因此 $L_2$ 正则的本质其实是为模型增加了**模型参数服从零均值正态分布**这一先验知识。

#### $L_1$和$L_2$正则化的区别

$L_1$ 正则化增加了所有权重 w 参数的绝对值之和逼迫更多 w 为零，也就是变稀疏。我们对稀疏规则趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，大部分特征 $x_i$ 都是和最终的输出 $y_i$ 没有关系或者不提供任何信息的。在最小化目标函数的时候考虑 $x_i$ 这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的特征权重反而会被考虑，从而干扰了对正确 $y_i$ 的预测。$L_1$ 正则化的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些无用的特征，也就是把这些特征对应的权重置为0。

$L_2$正则化中增加所有权重 w 参数的平方之和，逼迫所有 w 尽可能趋向零但不为零（$L_2$的导数趋于零）。因为在未加入 $L_2$正则化发生过拟合时，拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大，在某些很小的区间里，函数值的变化很剧烈，也就是某些 w 值非常大。为此，$L_2$正则化的加入就惩罚了权重变大的趋势。

$L_1$正则化指权值向量中各个元素的绝对值之和，$L_2$正则化是指权值向量中各个元素的平方和再求平方根。$L_1$正则会产生稀疏解，$L_2$正则会产生比较小的解。以二维为例，$L_1$正则化项和误差项的交点常出现在坐标轴上，是个菱形，$w_1$或$w_2$为0，即权值向量中有零值元素，而$L_2$正则化项与误差项的交点常出现在某个象限中，是个圆，$w_1$和$w_2$均非0。

![](/img/面试问题总结/L1L2.jpg)

### 工程上，怎么实现逻辑回归的并行化？有哪些并行化工具？

逻辑回归的并行化最主要的就是**对目标函数梯度计算的并行化**。目标函数的梯度向量计算中只需要进行向量间的点乘和相加，可以很容易将每个迭代过程拆分成相互独立的计算步骤，由不同的节点进行独立计算，然后归并计算结果。算法的并行化有两种：**无损并行化和有损并行化**。
基于Batch的算法(Batch-GD, LBFGS, OWLQN)都是可以进行无损的并行化的。而基于SGD的算法（Ad Predictor， FTRL－Proximal）都只能进行有损的并行化。

1. 无损的并行化：算法天然可以并行，并行只是提高了计算的速度和解决问题的规模，但和正常执行的结果是一样的。
2. 有损的并行化：算法本身不是天然并行的，需要对算法做一些近似来实现并行化，这样并行化之后的双方和正常执行的结果并不一致，但是相似的。

并行化的工具有**MPI**和**OpenMP**。

### 逻辑回归与其他模型的比较

#### 与线性回归模型

逻辑回归是在线性回归的基础上加了一个 sigmoid 函数（非线形）映射，使得逻辑回归称为了一个优秀的分类算法。本质上来说，两者都属于广义线性模型，但他们两个要解决的问题不一样，逻辑回归解决的是分类问题，输出的是离散值，线性回归解决的是回归问题，输出的连续值。

sigmoid的作用：
+ 线性回归是在实数域范围内进行预测，而分类范围则需要在 [0,1]，逻辑回归减少了预测范围；
+ 线性回归在实数域上敏感度一致，而逻辑回归在 0 附近敏感，在远离 0 点位置不敏感，这个的好处就是模型更加关注分类边界，可以增加模型的鲁棒性。

#### 与最大熵模型

逻辑回归和最大熵模型本质上没有区别，最大熵在解决二分类问题时就是逻辑回归，在解决多分类问题时就是多项逻辑回归。

#### 与SVM模型

相同点：<br>
1. 都是分类算法，本质上都是寻找划分超平面
2. 都是监督学习算法
3. 都是判别式模型
4. 都可以通过核技巧的方法对非线性情况进行分类
5. 都能减少离群点的影响

不同点：<br>
1. 损失函数不同，LR是对数损失函数，SVM是合页损失函数。
2. LR对异常值敏感，SVM对异常值不敏感。解释：支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局。LR模型找到的那个超平面，是尽量让所有点都远离他，而SVM寻找的那个超平面，是只让最靠近中间分割线的那些点尽量远离，即只用到那些支持向量的样本。 支持向量机改变非支持向量样本并不会引起决策面的变化。逻辑回归中改变任何样本都会引起决策面的变化。
3. 对非线性问题的处理方式不同。解释：LR主要通过特征构造，特征组合(特征交叉)、特征离散化来处理非线性问题。SVM通常采用核函数来高效处理非线性问题。
4. 理论基础不一样。LR基于统计，而SVM基于严格的数学推导。
5. 输出不同。LR可以对每个样本点给出类别判断的概率值(或置信度)，SVM无法做到。
6. 计算复杂度不同。对于海量数据，SVM的效率较低，LR的效率比较高。解释：当样本较少，特征维数较低时，SVM和LR的运行时间均比较短，SVM较短一些。准确率的话，LR明显比SVM要高。当样本稍微增加些时，SVM运行时间开始增长，但是准确率赶超了LR。SVM时间虽长，但在可接受范围内。当数据量增长到20000时，特征维数增长到200时，SVM的运行时间剧烈增加，远远超过了LR的运行时间。但是准确率却和LR相差无几。(这其中主要原因是大量非支持向量参与计算，造成SVM的二次规划问题)
7. 防止过拟合的能力不同。解释：SVM模型中内含了L2正则，可有效防止过拟合。LR要自己添加正则项。
8. 对数据要求不同。SVM依赖于数据表达出的距离测度，所以需要对数据进行标准化处理，而LR不需要。
9. SVM会用核函数而LR一般不用核函数。
10. SVM自带结构风险最小化，LR则是经验风险最小化。
