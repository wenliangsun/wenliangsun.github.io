---
layout: post
title: "优化算法总结"
subtitle: "优化算法"
author: WenlSun"
header-style: text
tag:
  - 优化算法
---
[各优化算法的优缺点整理](https://blog.csdn.net/zhouhong0284/article/details/80232412)

## 梯度下降算法

### 一维梯度下降算法

$$
x_t=x_{t-1}-\eta f'(x)
$$

根据泰勒展开公式，可以得到下面的近似

$$
f(x+\epsilon )\approx f(x)+f'(x)\cdot \epsilon
$$

这里$f'(x)$是f在x点处的梯度，一维函数的梯度是一个标量，即导数。

### 多维梯度下降算法

$$
\boldsymbol {x := x-\eta \nabla f(x)}
$$

### 随机梯度下降

在深度学习⾥，⽬标函数通常是训练数据集中有关各个样本的损失函数的平均。设$f_i(x)$是有关索引为i的训练数据样本的损失函数，n是训练数据样本数，x是模型的参数向量，那么⽬标函数定义为 

$$
\boldsymbol {f(x)=\frac{1}{n}\sum_{i=1}^{n}f_i(x)}
$$

目标函数在x出的梯度计算为

$$
\boldsymbol {\nabla f(x)=\frac{1}{n}\sum_{i=1}^n\nabla f_i(x)}
$$

$$
\boldsymbol {x = x -\eta \nabla f_i(x)}
$$

随机梯度$\nabla f_i(x)$是对梯度$\nabla f(x)$的无偏估计。这意味着，平均来说，随机梯度是对梯度的⼀个良好的估计。

### 小批量随机梯度下降

$$
\boldsymbol {g_t= \nabla f_{B_t}(x_{t-1}) = \frac{1}{|B|}\sum_{i\in B_t} \nabla f_i(x_{t-1})}
$$

## 动量法

$$
\boldsymbol {v_t := \gamma v_{t-1}+ \eta_{t}g_{t}}
$$
$$
\boldsymbol {x_t := x_{t-1}-v_t}
$$

![](/img/面试问题总结/动量法.png)

## AdaGrad算法

在之前介绍过的优化算法中，⽬标函数⾃变量的每⼀个元素在相同时间步都使⽤同⼀个学习率来⾃我迭代。AdaGrad算法，它根据⾃变量在每个维度的梯度值的⼤小来调整各个维度上的学习率，从而避免统⼀的学习率难以适应所有维度的问题.

$$
\boldsymbol {s_t := s_{t-1}+g_t \bigodot g_t}
$$
$$
\boldsymbol {x_t := x_{t-1} - \frac{\eta}{\sqrt{s_t+\epsilon}}\bigodot g_t }
$$

其中$\bigodot$是按元素相乘，$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，这⾥开⽅、除法和乘法的运算都是按元素运算的。这些按元素运算使得⽬标函数⾃变量中每个元素都分别拥有⾃⼰的学习率。

特点:<br>
需要强调的是，小批量随机梯度按元素平⽅的累加变量$s_t$出现在学习率的分⺟项中。因此，如果⽬标函数有关⾃变量中某个元素的偏导数⼀直都较⼤，那么该元素的学习率将下降较快；反之，如果⽬标函数有关⾃变量中某个元素的偏导数⼀直都较小，那么该元素的学习率将下降较慢。然而，由于$s_t$⼀直在累加按元素平⽅的梯度，⾃变量中每个元素的学习率在迭代过程中⼀直在降低（或不变）。所以，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到⼀个有⽤的解。

## RMSProp算法

为了解决Adagrad算法中的问题，RMSProp算法对AdaGrad算法做了⼀点小小的修改。不同于AdaGrad算法⾥状态变量$s_t$是截⾄时间步t所有小批量随机梯度$g_t$按元素平⽅和，RMSProp算法将这些梯度按元素平⽅做指数加权移动平均。

$$
\boldsymbol {S_t := \gamma S_{t-1} + (1-\gamma)g_t \bigodot g_t}
$$
$$
\boldsymbol {X_t := X_{t-1}-\frac{\eta}{\sqrt{S_t+\epsilon}}\bigodot g_t}
$$

其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数 。因为RMSProp算法的状态变量$s_t$是对平⽅项$g_t\bigodot g_t$的指数加权移动平均，所以可以看作是最近$1/(1-\gamma )$个时间步的小批量随机梯度平⽅项的加权平均。如此⼀来，⾃变量每个元素的学习率在迭代过程中就不再⼀直降低（或不变）。

## AdaDelta 算法

除了RMSProp算法以外，另⼀个常⽤优化算法AdaDelta算法也针对AdaGrad算法在迭代后期可能较难找到有⽤解的问题做了改进。有意思的是，AdaDelta算法没有学习率这⼀超参数。
AdaDelta算法也像RMSProp算法⼀样，使⽤了小批量随机梯度$g_t$按元素平⽅的指数加权移动平均变量$s_t$。与RMSProp算法不同的是，AdaDelta算法还维护⼀个额外的状态变量$\Delta x_t$，其元素同样在时间步0时被初始化为0。我们使⽤$\Delta x_{t-1}$来计算⾃变量的变化量。最后，我们使⽤$\Delta x_t$来记录⾃变量变化量$g_t'$按元素平⽅的指数加权移动平均。
$$
\boldsymbol {S_t := \rho S_{t-1}+(1-\rho)g_t\bigodot g_t}
$$
$$
\boldsymbol {g_t' := \sqrt{\frac{\Delta X_{t-1}+\epsilon}{S_t+\epsilon}}\bigodot g_t}
$$
$$
\boldsymbol {X_t := X_{t-1}-g_t'}
$$
$$
\boldsymbol {\Delta X_t := \rho \Delta X_{t-1}+(1-\rho)g_t'\bigodot g_t'}
$$

可以看到，如不考虑$\epsilon$的影响，AdaDelta算法跟RMSProp算法的不同之处在于使⽤$\sqrt{\Delta X_{t-1}}$来替代超参数$\eta$。

## Adam算法

Adam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均。Adam算法使⽤了动量变量$v_t$和RMSProp算法中小批量随机梯度按元素平⽅的指数加权移动平均变量$s_t$，并在时间步0将它们中每个元素初始化为0。给定超参数$0\leq \beta_1 \le 1$（算法作者建议设为0.9），时间步t的动量变量$v_t$即小批量随机梯度$g_t$的指数加权移动平均。和RMSProp算法中⼀样，给定超参数$0 \leq \beta_2\le 1$（算法作者建议设为0.999），将小批量随机梯度按元素平⽅后的项$g_t\bigodot g_t$做指数加权移动平均得到$s_t$。

![](/img/面试问题总结/adam.png)

$$
\boldsymbol {t:=t+1}
$$
$$
\boldsymbol {v_t := \beta_1v_{t-1}+(1-\beta_1)g_t}
$$
$$
\boldsymbol {s_t := \beta_2s_{t-1}+(1-\beta_2)g_t\bigodot g_t}
$$
$$
\boldsymbol {\hat{v_t} := \frac{v_t}{1-\beta_1^t}}
$$
$$
\boldsymbol {\hat{s_t} := \frac{s_t}{1-\beta_2^t}}
$$
$$
\boldsymbol {g_t'= \frac{\eta \hat{v_t}}{\sqrt{\hat{s_t}}+\epsilon}}
$$
$$
\boldsymbol {x_t := x_{t-1}-g_t'}
$$
